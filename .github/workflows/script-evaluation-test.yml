# A nightly job which downloads script evaluation dumps from S3 and runs a regression test.
name: Script Evaluation Test
on:
  schedule:
    - cron 30 3 * * * # 3:30am every day 

jobs: 
  script-evaluation-test: 
    runs-on: [self-hosted, plutus-benchmark]

  steps:
    - name: Download and unzip dump files
      run: |
# nix-shell --run 'LOCAL_DIR=$HOME/mainnet-script-dump-downloaded ./scripts/s3-sync-unzip.sh s3://plutus/mainnet-script-dump/ *.event.bz2'
        nix develop --no-warn-dirty --accept-flake-config --command 
          'bash ./scripts/s3-sync-unzip.sh s3://plutus/mainnet-script-dump/ *.event.bz2'
      env: 
        LOCAL_DIR: $HOME/mainnet-script-dump-downloaded
        AWS_ACCESS_KEY_ID: {{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: {{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-1
        AWS_ENDPOINT_URL: https://s3.devx.iog.io

    - name: Run Script Evaluation Test
      # Run the test cases sequentially. This ensures we don't need to simultaneously store
      # multiple `ScriptEvaluationEvents`, which are large, in memory. Each test case
      # contains many script evaluation events, and those are run in parallel based on
      # the number of available processors.
      run: |
# nix-shell --run 'cabal update && EVENT_DUMP_DIR=$HOME/mainnet-script-dump-downloaded cabal v2-run plutus-ledger-api:evaluation-test -- --num-threads=1'
        nix develop --no-warn-dirty --accept-flake-config --command \
          'cabal update && cabal v2-run plutus-ledger-api:evaluation-test -- --num-threads=1'
      env: 
        EVENT_DUMP_DIR: $HOME/mainnet-script-dump-downloaded

